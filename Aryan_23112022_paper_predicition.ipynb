{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60122c77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6678af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import textstat\n",
    "import shap\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599a7b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c2759ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def explain_prediction(index, shap_vals, X_data, y_true, y_pred, class_names, feature_names):\n",
    "    print(f\"\\n📄 Sample {index + 1}:\")\n",
    "    predicted = class_names[y_pred[index]]\n",
    "#     actual = class_names[y_true.iloc[index]]\n",
    "    print(f\"Predicted: {predicted}\")\n",
    "    \n",
    "    # Get SHAP values for class 1 (Publishable)\n",
    "    sample_shap = shap_vals[1][index]\n",
    "    sample_features = X_data.iloc[index]\n",
    "    \n",
    "    # Get top contributing features by absolute SHAP value\n",
    "    top_idx = np.argsort(np.abs(sample_shap))[::-1]\n",
    "    \n",
    "    print(\"Top factors contributing to this decision:\")\n",
    "    for i in top_idx[:5]:  # Top 5 features\n",
    "        direction = \"+\" if sample_shap[i] > 0 else \"-\"\n",
    "        print(f\"  - {feature_names[i]}: {direction}{abs(sample_shap[i]):.3f}\")\n",
    "\n",
    "    print(\"\\n📝 Explanation:\")\n",
    "    if y_pred[index] == 1:\n",
    "        print(\"This paper is considered *Publishable* because it performs well on features like:\")\n",
    "        for i in top_idx[:3]:\n",
    "            if sample_shap[i] > 0:\n",
    "                print(f\"  • {feature_names[i]} (positive impact)\")\n",
    "        print(\"despite some weaker areas like:\")\n",
    "        for i in top_idx[:3]:\n",
    "            if sample_shap[i] < 0:\n",
    "                print(f\"  • {feature_names[i]} (negative impact)\")\n",
    "    else:\n",
    "        print(\"This paper is considered *Non-Publishable* mainly due to:\")\n",
    "        for i in top_idx[:3]:\n",
    "            if sample_shap[i] < 0:\n",
    "                print(f\"  • {feature_names[i]} (negative impact)\")\n",
    "        print(\"even though it had some positive points like:\")\n",
    "        for i in top_idx[:3]:\n",
    "            if sample_shap[i] > 0:\n",
    "                print(f\"  • {feature_names[i]} (positive impact)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc3069c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1361a76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- TEXT EXTRACTION ----------\n",
    "def extract_text(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() or \"\"\n",
    "        return text\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fbe5961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "    text = text.lower()\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5de4d258",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_sections(text):\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Convert text to lowercase for comparison\n",
    "    lines = text.splitlines()\n",
    "    \n",
    "    intro_start = -1\n",
    "    intro_end = -1\n",
    "    concl_start = -1\n",
    "    concl_end = -1\n",
    "\n",
    "    #\"Introduction\" starts\n",
    "    for i, line in enumerate(lines):\n",
    "        if 'introduction' in line.lower():\n",
    "            intro_start = i\n",
    "            break\n",
    "\n",
    "    #\"Methodology\" or \"Methods\" starts (end of intro)\n",
    "    for i in range(intro_start + 1, len(lines)):\n",
    "        if 'methodology' in lines[i].lower() or 'methods' in lines[i].lower():\n",
    "            intro_end = i\n",
    "            break\n",
    "\n",
    "    #\"Conclusion\", \"Results\", or \"Discussion\" starts\n",
    "    for i, line in enumerate(lines):\n",
    "        if 'conclusion' in line.lower() or 'results' in line.lower() or 'discussion' in line.lower():\n",
    "            concl_start = i\n",
    "            break\n",
    "\n",
    "    #\"References\" or \"Acknowledgments\" starts (end of conclusion)\n",
    "    for i in range(concl_start + 1, len(lines)):\n",
    "        if 'references' in lines[i].lower() or 'acknowledgments' in lines[i].lower():\n",
    "            concl_end = i\n",
    "            break\n",
    "\n",
    "    # Extract sections using line indices\n",
    "    \n",
    "    if intro_start != -1 and intro_end != -1:\n",
    "        intro_section = \"\\n\".join(lines[intro_start + 1:intro_end])\n",
    "    else:\n",
    "        intro_section = \"\"\n",
    "\n",
    "    if concl_start != -1 and concl_end != -1:\n",
    "        concl_section = \"\\n\".join(lines[concl_start + 1:concl_end])\n",
    "    else:\n",
    "        concl_section = \"\"\n",
    "\n",
    "    return intro_section, concl_section\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "324225ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_citation_features(text):\n",
    "\n",
    "    # Count numbered citations like [1], [2,3], etc. \n",
    "    \n",
    "    numbered_citations = 0\n",
    "    for i in range(len(text)):\n",
    "        if text[i] == '[':\n",
    "            j = i + 1\n",
    "            while j < len(text) and text[j] != ']':\n",
    "                j += 1\n",
    "            if j < len(text) and text[j] == ']':\n",
    "                inside = text[i+1:j]\n",
    "                if any(char.isdigit() for char in inside):\n",
    "                    numbered_citations += 1\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "    #   Count author-based citations like (Smith et al., 2020)\n",
    "    \n",
    "    \n",
    "    author_citations = 0\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        if text[i] == '(':\n",
    "            j = i + 1\n",
    "            while j < len(text) and text[j] != ')':\n",
    "                j += 1\n",
    "            if j < len(text):\n",
    "                content = text[i+1:j]\n",
    "                if 'et al.' in content.lower() and '20' in content:\n",
    "                    author_citations += 1\n",
    "            i = j\n",
    "        i += 1\n",
    "\n",
    "    total_citations = numbered_citations + author_citations\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    #   Find and count references section \n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    start_index = -1\n",
    "    if \"references\" in text_lower:\n",
    "        start_index = text_lower.index(\"references\")\n",
    "    elif \"bibliography\" in text_lower:\n",
    "        start_index = text_lower.index(\"bibliography\")\n",
    "\n",
    "    references_text = \"\"\n",
    "    if start_index != -1:\n",
    "        references_text = text[start_index:]\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    # Count number of reference entries (non-empty lines)\n",
    "    \n",
    "    \n",
    "    reference_lines = references_text.split('\\n')\n",
    "    reference_count = 0\n",
    "    for line in reference_lines:\n",
    "        if line.strip():  # only count non-empty lines\n",
    "            reference_count += 1\n",
    "            \n",
    "            \n",
    "\n",
    "    #  Calculate citation density per 1000 words ----\n",
    "    \n",
    "    \n",
    "    words = text.split()\n",
    "    total_words = len(words)\n",
    "    if total_words > 0:\n",
    "        density = (total_citations / total_words) * 1000\n",
    "        density = round(density, 2)\n",
    "    else:\n",
    "        density = 0.0\n",
    "\n",
    "    return total_citations, reference_count, density\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fdb6201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#        IMPROVEMENT DETECTION FEATURE\n",
    "comparison_phrases = [\n",
    "    'better than', 'outperforms', 'compared to', 'improves upon',\n",
    "    'achieves higher', 'higher accuracy', 'lower error', 'state-of-the-art',\n",
    "    'compared with', 'previous methods', 'surpasses', 'yields better results'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58df9c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_improvement_statements(text):\n",
    "    text = text.lower()\n",
    "    return int(any(phrase in text for phrase in comparison_phrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b8488aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "for label, folder in [('Publishable', 'Publishable'), ('Non-Publishable', 'Non-Publishable')]:\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(folder, file)\n",
    "            text = extract_text(pdf_path)\n",
    "            if not text:\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Section extraction\n",
    "            \n",
    "            intro, concl = get_sections(text)\n",
    "            if intro and concl:\n",
    "                intro_embedding = model.encode(intro)\n",
    "                concl_embedding = model.encode(concl)\n",
    "                similarity = np.dot(intro_embedding, concl_embedding)\n",
    "            else:\n",
    "                similarity = 0\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "            # Readability\n",
    "            \n",
    "            cleaned_text = preprocess_text(text)\n",
    "            flesch_kincaid = textstat.flesch_kincaid_grade(text)\n",
    "            dale_chall = textstat.dale_chall_readability_score(text)\n",
    "            ari = textstat.automated_readability_index(text)\n",
    "            word_count = len(cleaned_text.split())\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Citation features\n",
    "            \n",
    "            in_text_cit, ref_count, cit_density = get_citation_features(text)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Improvement detection\n",
    "            \n",
    "            improvement_flag = check_improvement_statements(concl)\n",
    "            \n",
    "            \n",
    "            \n",
    "            data.append({\n",
    "                'similarity': similarity,\n",
    "                'flesch_kincaid': flesch_kincaid,\n",
    "                'dale_chall': dale_chall,\n",
    "                'ari': ari,\n",
    "                'word_count': word_count,\n",
    "                'in_text_cit': in_text_cit,\n",
    "                'ref_count': ref_count,\n",
    "                'cit_density': cit_density,\n",
    "                'improvement_flag': improvement_flag,\n",
    "                'label': 1 if label == 'Publishable' else 0\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a2f4bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9793a3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n",
      "F1 Score: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Feature set\n",
    "features = ['similarity', 'flesch_kincaid', 'dale_chall', 'ari',\n",
    "            'word_count', 'in_text_cit', 'ref_count', 'cit_density',\n",
    "            'improvement_flag']\n",
    "X = df[features]\n",
    "y = df['label']\n",
    "\n",
    "\n",
    "\n",
    "# Train/test split & model\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "642bab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\nFeature Importances:\")\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': clf.feature_importances_\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "# print(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b4c8bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e340b613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📄 Sample 1:\n",
      "Predicted: Publishable\n",
      "Top factors contributing to this decision:\n",
      "  - flesch_kincaid: -0.000\n",
      "  - similarity: -0.000\n",
      "\n",
      "📝 Explanation:\n",
      "This paper is considered *Publishable* because it performs well on features like:\n",
      "despite some weaker areas like:\n",
      "\n",
      "📄 Sample 2:\n",
      "Predicted: Publishable\n",
      "Top factors contributing to this decision:\n",
      "  - flesch_kincaid: +0.127\n",
      "  - similarity: -0.127\n",
      "\n",
      "📝 Explanation:\n",
      "This paper is considered *Publishable* because it performs well on features like:\n",
      "  • flesch_kincaid (positive impact)\n",
      "despite some weaker areas like:\n",
      "  • similarity (negative impact)\n",
      "\n",
      "📄 Sample 3:\n",
      "Predicted: Publishable\n",
      "Top factors contributing to this decision:\n",
      "  - flesch_kincaid: +0.019\n",
      "  - similarity: -0.019\n",
      "\n",
      "📝 Explanation:\n",
      "This paper is considered *Publishable* because it performs well on features like:\n",
      "  • flesch_kincaid (positive impact)\n",
      "despite some weaker areas like:\n",
      "  • similarity (negative impact)\n"
     ]
    }
   ],
   "source": [
    "explainer = shap.TreeExplainer(clf)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "# Choose the class index: 1 = Publishable, 0 = Non-Publishable\n",
    "class_names = ['Non-Publishable', 'Publishable']\n",
    "feature_names = X.columns.tolist()\n",
    "y_pred = clf.predict(X)\n",
    "for i in range(3):\n",
    "    explain_prediction(i, shap_values, X, y, y_pred, class_names, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8d3f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9454909d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df99e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "239853ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_unlabeled(row, clf, feature_means, prediction):\n",
    "    reasons = []\n",
    "    \n",
    "    # If the prediction is \"Publishable\" (1), explain positive contributing factors\n",
    "    if prediction == 1:\n",
    "        if row['similarity'] > feature_means['similarity']:\n",
    "            reasons.append(\"high similarity between Introduction and Conclusion\")\n",
    "        if row['ref_count'] > feature_means['ref_count']:\n",
    "            reasons.append(\"strong reference section\")\n",
    "        if row['cit_density'] > feature_means['cit_density']:\n",
    "            reasons.append(\"good citation density\")\n",
    "        if row['improvement_flag'] == 1:\n",
    "            reasons.append(\"mentions improvement over previous work\")\n",
    "        if row['flesch_kincaid'] < feature_means['flesch_kincaid']:\n",
    "            reasons.append(\"uses easy-to-understand language\")\n",
    "        if row['dale_chall'] < feature_means['dale_chall']:\n",
    "            reasons.append(\"clear readability (low Dale-Chall score)\")\n",
    "\n",
    "    # If the prediction is \"Non-Publishable\" (0), explain negative contributing factors\n",
    "    else:\n",
    "        if row['similarity'] <= feature_means['similarity']:\n",
    "            reasons.append(\"low similarity between Introduction and Conclusion\")\n",
    "        if row['ref_count'] <= feature_means['ref_count']:\n",
    "            reasons.append(\"weak reference section\")\n",
    "        if row['cit_density'] <= feature_means['cit_density']:\n",
    "            reasons.append(\"low citation density\")\n",
    "        if row['improvement_flag'] == 0:\n",
    "            reasons.append(\"does not mention improvement over previous work\")\n",
    "        if row['flesch_kincaid'] > feature_means['flesch_kincaid']:\n",
    "            reasons.append(\"uses complex language (high Flesch-Kincaid grade)\")\n",
    "        if row['dale_chall'] > feature_means['dale_chall']:\n",
    "            reasons.append(\"difficult readability (high Dale-Chall score)\")\n",
    "\n",
    "    # Return a joined string of reasons\n",
    "    return \"; \".join(reasons) if reasons else \"general poor quality across features\" if prediction == 0 else \"general good quality across features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92c92ad1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n",
      "F1 Score: 1.00\n",
      "\n",
      "Unlabeled predictions saved to 'unlabeled_predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    data = []\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    # Load labeled data (Publishable, Non-Publishable)\n",
    "    for label, folder in [('Publishable', 'Publishable'), ('Non-Publishable', 'Non-Publishable')]:\n",
    "        for file in os.listdir(folder):\n",
    "            if file.endswith('.pdf'):\n",
    "                pdf_path = os.path.join(folder, file)\n",
    "                text = extract_text(pdf_path)\n",
    "                if not text:\n",
    "                    continue\n",
    "                \n",
    "                # Section extraction\n",
    "                intro, concl = get_sections(text)\n",
    "                similarity = model.encode(intro) @ model.encode(concl).T if intro and concl else 0\n",
    "\n",
    "                # Readability\n",
    "                cleaned_text = text  # Assuming cleaned text is just the extracted text here\n",
    "                flesch_kincaid = textstat.flesch_kincaid_grade(text)\n",
    "                dale_chall = textstat.dale_chall_readability_score(text)\n",
    "                ari = textstat.automated_readability_index(text)\n",
    "                word_count = len(cleaned_text.split())\n",
    "\n",
    "                # Citation features\n",
    "                in_text_cit, ref_count, cit_density = get_citation_features(text)\n",
    "\n",
    "                # Improvement detection\n",
    "                improvement_flag = check_improvement_statements(concl)\n",
    "\n",
    "                data.append({\n",
    "                    'similarity': similarity,\n",
    "                    'flesch_kincaid': flesch_kincaid,\n",
    "                    'dale_chall': dale_chall,\n",
    "                    'ari': ari,\n",
    "                    'word_count': word_count,\n",
    "                    'in_text_cit': in_text_cit,\n",
    "                    'ref_count': ref_count,\n",
    "                    'cit_density': cit_density,\n",
    "                    'improvement_flag': improvement_flag,\n",
    "                    'label': 1 if label == 'Publishable' else 0,\n",
    "                    'pdf_name': file\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Feature set\n",
    "    features = ['similarity', 'flesch_kincaid', 'dale_chall', 'ari',\n",
    "                'word_count', 'in_text_cit', 'ref_count', 'cit_density',\n",
    "                'improvement_flag']\n",
    "    X = df[features]\n",
    "    y = df['label']\n",
    "\n",
    "    # Train/test split & model\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Calculate feature means for explanation (after training)\n",
    "    feature_means = X_train.mean()\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "    print(f\"F1 Score: {f1_score(y_test, y_pred):.2f}\")\n",
    "\n",
    "    # ----------- UNLABELED PREDICTIONS ----------\n",
    "    unlabeled_data = []\n",
    "    unlabeled_folder = 'Unlabeled'\n",
    "    for file in os.listdir(unlabeled_folder):\n",
    "        if file.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(unlabeled_folder, file)\n",
    "            text = extract_text(pdf_path)\n",
    "            if not text:\n",
    "                continue\n",
    "            \n",
    "            # Section extraction\n",
    "            intro, concl = get_sections(text)\n",
    "            similarity = model.encode(intro) @ model.encode(concl).T if intro and concl else 0\n",
    "\n",
    "            # Readability\n",
    "            cleaned_text = text  # Assuming cleaned text is just the extracted text here\n",
    "            flesch_kincaid = textstat.flesch_kincaid_grade(text)\n",
    "            dale_chall = textstat.dale_chall_readability_score(text)\n",
    "            ari = textstat.automated_readability_index(text)\n",
    "            word_count = len(cleaned_text.split())\n",
    "\n",
    "            # Citation features\n",
    "            in_text_cit, ref_count, cit_density = get_citation_features(text)\n",
    "\n",
    "            # Improvement detection\n",
    "            improvement_flag = check_improvement_statements(concl)\n",
    "\n",
    "            unlabeled_data.append({\n",
    "                'pdf_name': file,\n",
    "                'similarity': similarity,\n",
    "                'flesch_kincaid': flesch_kincaid,\n",
    "                'dale_chall': dale_chall,\n",
    "                'ari': ari,\n",
    "                'word_count': word_count,\n",
    "                'in_text_cit': in_text_cit,\n",
    "                'ref_count': ref_count,\n",
    "                'cit_density': cit_density,\n",
    "                'improvement_flag': improvement_flag\n",
    "            })\n",
    "\n",
    "    unlabeled_df = pd.DataFrame(unlabeled_data)\n",
    "\n",
    "    # Predict unlabeled data\n",
    "    unlabeled_predictions = clf.predict(unlabeled_df[features])\n",
    "\n",
    "    # ----------- EXPLAIN PREDICTIONS ----------\n",
    "    unlabeled_df['Prediction'] = unlabeled_predictions\n",
    "    unlabeled_df['Reason'] = unlabeled_df.apply(\n",
    "        lambda row: explain_unlabeled(row, clf, feature_means, row['Prediction']), axis=1\n",
    "    )\n",
    "\n",
    "    # Filter out unnecessary columns and retain only Name, Prediction, and Reason\n",
    "    final_df = unlabeled_df[['pdf_name', 'Prediction', 'Reason']]\n",
    "\n",
    "    # Save to CSV\n",
    "    final_df.to_csv('unlabeled_predictions.csv', index=False)\n",
    "    print(\"\\nUnlabeled predictions saved to 'unlabeled_predictions.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f798128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec396cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dd67df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c06f84c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796f654d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bed097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3823fae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf3dd84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb8e943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy==1.24.4 numba==0.57.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f362c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10baf0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247c40d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81807479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d307c50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76fd564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eec17a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72726234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3846c627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d07ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ef26bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
